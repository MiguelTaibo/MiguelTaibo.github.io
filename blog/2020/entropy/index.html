<!DOCTYPE html>
<html lang="en">

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Miguel Taibo Martínez


  | Entropy and Differential Entropy

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>M</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="https://migueltaibo.github.io/blog/2020/entropy/">


<!-- Dark Mode -->
<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://MiguelTaibo.github.io/">
       <span class="font-weight-bold">Miguel</span> Taibo  Martínez
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      





<div class="post">

  <header class="post-header">
    <h1 class="post-title">Entropy and Differential Entropy</h1>
    <p class="post-meta">February 11, 2020</p>
    <p class="post-tags">
      <a href="/blog/2020"> <i class="fas fa-calendar fa-sm"></i> 2020 </a>
      

      

    </p>
  </header>

  <article class="post-content">
    <h1 id="entropy">Entropy</h1>

<p>In information theory, the entropy is the information or uncertainty inherent of a random variable.</p>

<p>Imagine we have a random variable \(X\) with \(N\) possible outcomes with probability \(P(x_1), P(x_2), ... P(x_N)\). Entropy of \(X\) is defined as:</p>

<p>\begin{equation}
    H(X)=-\sum_{i=1}^N P(x_i)\log(P(x_i))
\end{equation}</p>

<p>Now, imagine with have a continious random variable \(Y\) uniform in between 0 and 1, i.e., \(Y\sim U(0,1)\). Defined by its  probability density function, \(f_Y(y)\):</p>

\[f_Y(y) = \left\{ 
\begin{matrix}
0 &amp; y&lt;0 \\
1 &amp; 0\leq y &lt; 1 \\
0 &amp; 1 \leq 1 \\
\end{matrix}    
\right.\]

<p>It is impossible to define a probability, \(P(y)\), as done before, we would be forcer to define r.v. \(Y\) to be in a neighbourhood of \(y\), \(I=(y-\delta/2,y+\delta/2)\). Therefore, we would have \(P(Y\in I)=\delta\) where \(\delta\to0\), and to cover the hole range of \(Y\) we have \(N=1/\delta\) segments. Therefore, compute the entropy of \(Y\) as:</p>

\[H(Y) = \lim_{\delta\to0}-\sum_{i=1}^{1/\delta} P(Y\in I_i)\log(P(Y\in I_i)) = \lim_{\delta\to0}-\frac1\delta \delta \log(\delta) = -\lim_{\delta\to0}\log(\delta)=\infty\]

<p>As we can see, in order to compute the entropy, we must break the continious random variable \(Y\) into infinite intervals with differential width. This makes the entropy to be infinite always, no matter which random variable we are talking about while it is continious. This motivates the definition of the differential entropy.</p>

<h1 id="differential-entropy">Differential Entropy</h1>

<p>We can prove the previous conclusion by developing the same development made to any continious random variable \(Y\) without restricting it to the 0,1 uniform. Therefore, we have any probability density function, \(f_Y(y)\).</p>

<p>In the same way than before we define \(P(Y\in I)\) with \(I=(y-\delta/2,y+\delta/2)\), and we have that, if \(\delta\to0\)</p>

\[P(Y\in I_i) = \int_{y_i-\delta}^{y_i+\delta} f_Y(\eta)d\eta ~~~ \Rightarrow ~~~ \lim_{\delta\to0} P(Y\in I_i) = \lim_{\delta\to0} \delta f_Y(y_i)\]

<p>(if the neighbourhood is small enough, its probability density can be enough well approximated to a constant). Therefore:</p>

\[H(Y) = -\sum_{\forall i} P(Y\in I_i) \log(P(Y\in I_i) )  \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\]

\[= -\lim_{\delta\to0}\sum_{\forall i} \delta f_Y(\eta) log(\delta f_Y(\eta))d\eta \;\;\;\;\;\;\;\]

\[= -\lim_{\delta\to0}\int_{-\infty}^\infty f_Y(\eta) log(\delta f_Y(\eta))d\eta \;\;\;\;\;\;\;\]

\[\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;=  -\lim_{\delta\to0}\int_{-\infty}^\infty f_Y(\eta) log(f_Y(\eta))d\eta  -\lim_{\delta\to0}\int_{-\infty}^\infty f_Y(\eta) log(\delta)d\eta\]

\[\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;= \int_{-\infty}^\infty f_Y(\eta) log(f_Y(\eta))d\eta -\lim_{\delta\to0} \log(\delta)\int_{-\infty}^\infty f_Y(\eta) d\eta\]

\[= \int_{-\infty}^\infty f_Y(\eta) log(f_Y(\eta))d\eta  -\lim_{\delta\to0} \log(\delta)\]

<p>As we can see, we obtain a formula with two terms, the first one, is a finite value, which depends on the probability density function of \(Y\), \(f_Y(y)\). The second one, infinite, is exactly equal to the one obtained before, when evaluating the entropy of the uniform 0,1 ditribution. Additionally, if we subssitute the probability density function of a 0,1 uniform, we obtain the same expression than before.</p>

<p>We use this result to define the <strong>diferential entropy</strong>, which is, in fact, the entropy compared to a uniform ditribution with width 1:</p>

<p>\begin{equation}
    h(y)=\mathbf{E}[-\log(f_Y(y))]=-\int_{-\infty}^\infty f_Y(\eta)\log(f_Y(\eta))d\eta
\end{equation}</p>

<p>Now we are going to get the differential entropy for the most common pdf:</p>

<p>1) given a uniform random variable, \(Y\sim U(a,b)\), its differential entropy is:</p>

\[h(y) = -\int_{-\infty}^\infty f_Y(\eta)\log(f_Y(\eta))d\eta = -\int_{a}^b \frac1{b-a}\log(b-a)d\eta = \log(b-a)\]

<p>2) given a gaussian random variable, \(Y\sim \mathcal N(\mu,\sigma)\), its differential entropy is:</p>

\[h(y) = -\int_{-\infty}^\infty f_Y(\eta)\log(f_Y(\eta))d\eta = -\int_{-\infty}^\infty  \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(\eta-\mu)^2}{2\sigma^2}} \ln\left(\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(\eta-\mu)^2}{2\sigma^2}}\right) d\eta = \frac12\ln(2\pi e\sigma^2)\]


  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    © Copyright 2022 Miguel Taibo Martínez.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.

    
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
